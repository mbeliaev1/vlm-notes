---
title: Direct Preference Optimization Your Language Model is Secretly a Reward Model
date: 2025-05-24
tags:
  - alignment
paper: https://arxiv.org/abs/2305.18290
code: https://huggingface.co/docs/trl/main/en/dpo_trainer
year: 2024
star: true
---
Casts RLHF as a closed-form log-ratio loss on static preference pairs, cheap to optimize.