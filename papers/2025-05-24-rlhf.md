---
title: Fine-Tuning Language Models from Human Preferences
date: 2025-05-24
tags:
  - alignment
  - rl
paper: https://arxiv.org/abs/1909.08593
code: https://github.com/openai/lm-human-preferences?tab=readme-ov-file
year: 2019
star: true
---
Original “Fine-Tuning from Human Preferences” paper; establishes reward-model + policy-gradient recipe still used today. 
