---
title: Contrastive Preference Optimization Pushing the Boundaries of LLM Performance in Machine Translation
date: 2025-05-24
tags:
  - alignment
paper: https://arxiv.org/abs/2401.08417
code: https://huggingface.co/docs/trl/en/cpo_trainer
year: 2024
star:
---
Contrastive pairs train models to avoid “okay-but-not-perfect” outputs and lifts machine translation quality with only 22k sentences. Based on contrastive preference learning, which avoids learning the advantage function through RL by optimizing the log likelihood of the policy directly with human feedback pairs. 

