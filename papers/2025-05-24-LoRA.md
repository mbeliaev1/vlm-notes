---
title: LoRA Low-Rank Adaptation of Large Language Models
date: 2025-05-24
tags:
  - post-training
  - sft
paper: https://arxiv.org/abs/2106.09685
code: https://huggingface.co/docs/peft/index
year: 2021
star: true
---
Freezes the base VLM and learns a low rank-decomposition. Remains one of the top methods for parameter efficient fine tuning used for adapting pretrained models to specific downstream tasks.  