---
title: Training language models to follow instructions with human feedback
date: 2025-05-24
tags:
  - alignment
  - rl
paper: https://arxiv.org/abs/2203.02155
code: https://github.com/openai/following-instructions-human-feedback
year: 2022
star: true
---

First demonstration that RLHF can make a 1.3 B model outperform 175 B GPT-3 at following instructions. Evidence that alignment pays off even on small footprints.