---
title: V-JEPA 2 Self-Supervised Video Models Enable Understanding, Prediction and Planning
date: 2025-06-30
tags:
paper: https://arxiv.org/html/2506.09985v1
code: https://github.com/facebookresearch/vjepa2
year: 2025
star: true
---
Yann LeCun's is one step closer to materializing his vision, learning a world model from  large amounts of unstructured visual data. This work scales the original concept JEPA, using 1M hours and 1M images to pretain a 1B-parameter vision transformer (ViT) without language supervision.  Specifically, the ViT is trained to predict a representation of a video from a view of that video that has been masked by removing random patches. This improves the model's spatiotemporal understanding, and allows for quick adaptation to real world robotic manipulation tasks. Notably, while this vision encoder is trained without textual grounding, it can be aligned with an LLM using the visual instruction tuning procedure (LLavA framework), leading to SOTA results on multiple VQA benchmarks. It will be interesting to see if this type of *video* encoder training framework will be popularized, replacing current *image* encoders such as CLIP and SigLIP. 