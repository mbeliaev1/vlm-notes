---
title: Inference Compute-Optimal Video Vision Language Models
date: 2025-06-01
tags: 
paper: https://arxiv.org/pdf/2505.18855
code: https://github.com/tt6746690/vvlm_inference_scaling
year: "2025"
star:
---
Derive scaling laws and a closed-form recipe for allocating per-example inference compute budget across VLM size, number of frames, and visual token density. Interesting analysis focused on inference constraints, not just model performance. Highlight that all three factors obey sub-linear power laws, and that joint scaling is crucial.