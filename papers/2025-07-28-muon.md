---
title: Muon is Scalable for LLM Training
date: 2025-07-28
tags:
paper: https://arxiv.org/abs/2502.16982
code: 
year: 2025
star: 
---
Coming from Moonshot AI labs, this work scales the "Muon"-optimizer by adding weight-decay and per-parameter scale corrections. This results in 2x compute efficiency over AdamW, achieving a new SOTA in LLM training for FLOPs vs. performance. Interesting analysis showing that Muon explores broader weight directions. Intuitively, this makes sense as the algorithm is equally considering all singular values that contribute positively to the gradient, allowing for more "exploration". 
